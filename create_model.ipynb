{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install Keras\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Bidirectional\n",
    "from keras import models \n",
    "import keras\n",
    "from scipy.signal import blackman\n",
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateModel():\n",
    "    \n",
    "    def original_data_visualization(self,df, word_list, first_period):\n",
    "        '''\n",
    "        A function that displays the original tf -idf values for word vectors\n",
    "        '''\n",
    "        last_period = len(df.iloc[1,1]) - first_period\n",
    "\n",
    "        plt.style.use('seaborn-darkgrid')\n",
    "        # create a color palette\n",
    "        palette = plt.get_cmap('Set1')\n",
    "\n",
    "        for i in range(len(word_list)):\n",
    "            plt.plot(df['tf-idf'][df['word'] == word_list[i]].to_list()[0][first_period:], marker='o',linestyle='dashed', color=palette(i),linewidth=1, alpha=0.9, label=word_list[i])\n",
    "        # Add legend\n",
    "        plt.legend(loc=1, ncol=1)\n",
    "\n",
    "        plt.gcf().set_size_inches(15.5, 10.5, forward=True)  \n",
    "        plt.savefig('image_data/Chart original data.png') \n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "    def words_highest_text_rank(self,df):\n",
    "        '''\n",
    "        A function that returns the 10 words with the highest text rank\n",
    "        '''\n",
    "        word_list = df['word'][df['word_rank'] <= 10].to_list()\n",
    "        return word_list\n",
    "    \n",
    "    def flat_split_sequences_validation(self,sequences, n_steps):\n",
    "        '''\n",
    "        A function that receives sequence and number_steps.\n",
    "        Distribution of each vector n_steps=50Distribution of each vector n_steps=50 \n",
    "        and divides the data into train = 60%, test = 20%, validation = 20%.\n",
    "        '''\n",
    "        X_train, y_train, X_test, y_test, X_valid, y_valid  = list(), list(), list(), list(), list(), list()\n",
    "\n",
    "        for word in range(sequences.shape[0]):\n",
    "            X, y = list(), list()\n",
    "            for i in range(sequences.shape[1]):\n",
    "                # find the end of this pattern\n",
    "                end_ix = i + n_steps\n",
    "                # check if we are beyond the dataset\n",
    "                if end_ix > sequences.shape[1]-1:\n",
    "                    break\n",
    "                # gather input and output parts of the pattern  \n",
    "                seq_x, seq_y = sequences[word][i:end_ix], sequences[word][end_ix]\n",
    "                X.append(seq_x)\n",
    "                y.append(seq_y)\n",
    "\n",
    "\n",
    "            # Split vectors for  60% train, 20% test and 20% valid           \n",
    "            train_split = int(len(X)*0.6)\n",
    "            val_split = int(len(X)*0.2)\n",
    "\n",
    "            X_train.append(X[:train_split])\n",
    "            X_valid.append(X[train_split : (train_split+val_split)])\n",
    "            X_test.append(X[(train_split+val_split):])\n",
    "\n",
    "            y_train.append(y[:train_split])\n",
    "            y_valid.append(y[train_split:(train_split+val_split)])\n",
    "            y_test.append(y[(train_split+val_split):]) \n",
    "\n",
    "        X_train, y_train, X_test, y_test, X_valid, y_valid = array(X_train), array(y_train), array(X_test), array(y_test), array(X_valid), array(y_valid)\n",
    "\n",
    "        return X_train, y_train, X_test, y_test, X_valid, y_valid \n",
    "    \n",
    "    def get_lenght(self, X_train, X_valid, y_test):\n",
    "        '''\n",
    "        Data lengths after splitting data for visualization\n",
    "        '''\n",
    "        y_train_lenght = len(X_train[0])\n",
    "        y_valid_lenght = len(X_valid[0])\n",
    "        y_test_lenght = len(y_test[0])\n",
    "        print('y_train lenght :', y_train_lenght,', y_valid lenght :', y_valid_lenght, ', y_test lenght :', y_test_lenght)\n",
    "        return y_train_lenght, y_valid_lenght, y_test_lenght\n",
    "    \n",
    "    def reshape_data(self,X_train, y_train, X_test, y_test, X_valid, y_valid, n_steps):    \n",
    "        '''\n",
    "        Reshape data after split input_shape=(50, 1)\n",
    "        '''   \n",
    "        X_train = X_train.reshape(X_train.shape[0]*X_train.shape[1],n_steps,1)\n",
    "        y_train = y_train.reshape(y_train.shape[0]*y_train.shape[1],1)\n",
    "\n",
    "        X_valid = X_valid.reshape(X_valid.shape[0]*X_valid.shape[1],n_steps,1)\n",
    "        y_valid = y_valid.reshape(y_valid.shape[0]*y_valid.shape[1],1)\n",
    "\n",
    "        X_test =  X_test.reshape(X_test.shape[0]*X_test.shape[1],n_steps,1)\n",
    "        y_test =  y_test.reshape(y_test.shape[0]*y_test.shape[1],1)\n",
    "\n",
    "        return X_train, y_train, X_test, y_test, X_valid, y_valid \n",
    "    \n",
    "    def get_model_LSTM(self, n_features,n_steps, name_loss):\n",
    "        '''\n",
    "        Function that accepts number_features, number_steps, name_loss as arguments\n",
    "        And creates an LSTM network\n",
    "        '''\n",
    "        model = Sequential()\n",
    "                                                              #input_shape=(50, 1)\n",
    "        model.add(LSTM(units=20, activation=None,return_sequences=True,input_shape=(n_steps, n_features)))\n",
    "        model.add(LSTM(20,activation=None,return_sequences=True))\n",
    "        model.add(LSTM(20,activation=None,return_sequences=False))\n",
    "        model.add(Dense(n_features))\n",
    "        model.compile(optimizer='adam', loss=name_loss) \n",
    "        return model\n",
    "    \n",
    "    def get_model_Bidirectional(self, n_features,n_steps, name_loss):\n",
    "        '''\n",
    "        Function that accepts number_features, number_steps, name_loss as arguments\n",
    "        And creates an Bidirectional network\n",
    "        '''\n",
    "        model = Sequential()                                                         #input_shape=(50, 1)\n",
    "        model.add(Bidirectional(LSTM(20, return_sequences=False, activation='relu'), input_shape=(n_steps, n_features)))\n",
    "        model.add(Dense(n_features))\n",
    "        model.compile(optimizer='adam',loss=name_loss)\n",
    "        return model\n",
    "    \n",
    "    def fit_modle(self,model,X_train,y_train,X_valid,y_valid, y_train_lenght):\n",
    "        '''\n",
    "        fit the LSTM network\n",
    "        '''\n",
    "        n_features = X_train.shape[2]\n",
    "        vector_size = y_train_lenght\n",
    "        # fit model                                      \n",
    "        history = model.fit(X_train, y_train, epochs=3, batch_size = vector_size, shuffle=False,validation_data=(X_valid, y_valid),verbose=1)\n",
    "        return history\n",
    "    \n",
    "    def save_modle(self,name_modle,name_history,history):\n",
    "        '''\n",
    "        A function that preserves the model and the values of the loss function\n",
    "        '''\n",
    "        model.save(name_modle)\n",
    "        np.save(name_history+'.npy',history.history)\n",
    "        \n",
    "    def load_modle(self,name_modle,name_history):\n",
    "        '''\n",
    "        A function that imports the model and the values of the loss function\n",
    "        '''\n",
    "        load_history=np.load(name_history+'.npy',allow_pickle='TRUE').item()\n",
    "        reconstructed_model = models.load_model(name_modle)\n",
    "        reconstructed_model.summary()\n",
    "        return reconstructed_model, load_history\n",
    "    \n",
    "    def model_loss(self, load_history,name_loss):\n",
    "        '''\n",
    "        Function that displays a graph of the results of the loss function\n",
    "        '''\n",
    "        plt.plot(load_history['loss'])\n",
    "        plt.plot(load_history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel(name_loss+' loss')\n",
    "        plt.xlabel('epochs')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "    def get_data_predict(self, df00, n_steps):\n",
    "        '''\n",
    "        A function that returns the input data for model prediction\n",
    "        '''\n",
    "        full_dataset = pd.DataFrame(df00['tf-idf_after_norm'].to_list()).iloc[:,:].values\n",
    "        # Call to function flat_split_sequences_validation\n",
    "        full_X_train, full_y_train, full_X_test, full_y_test, full_X_valid, full_y_valid = self.flat_split_sequences_validation(full_dataset, n_steps)\n",
    "        full_X_train, full_y_train, full_X_test, full_y_test, full_X_valid, full_y_valid= self.reshape_data(full_X_train, full_y_train, full_X_test, full_y_test, full_X_valid, full_y_valid, n_steps)\n",
    "        return full_X_train, full_y_train, full_X_test, full_y_test, full_X_valid, full_y_valid\n",
    "    \n",
    "    def prediction_valid(self,reconstructed_model, full_X_valid):\n",
    "        '''\n",
    "        make a prediction valid\n",
    "        '''\n",
    "        prediction_valid = reconstructed_model.predict(full_X_valid, verbose=0)\n",
    "        return prediction_valid\n",
    "        \n",
    "    def prediction_test(self,reconstructed_model,full_X_test):\n",
    "        '''\n",
    "        make a prediction test\n",
    "        '''\n",
    "        prediction_test = reconstructed_model.predict(full_X_test, verbose=0)\n",
    "        return prediction_test\n",
    "        \n",
    "    def predictive_data_into_dataframe(self,df, prediction_valid,prediction_test, y_valid_lenght, y_test_lenght ):\n",
    "        '''\n",
    "        A function that puts forecast data into a data frame\n",
    "        '''\n",
    "        prediction_test_list = []\n",
    "        prediction_valid_list = []\n",
    "\n",
    "        for i in range(len(df['word'].to_list())):\n",
    "\n",
    "                word_index = df[df['word'] == df['word'].to_list()[i]].index[0]\n",
    "\n",
    "                valid_start = y_valid_lenght*word_index\n",
    "                prediction_valid_list.append(prediction_valid[valid_start:valid_start + y_valid_lenght]) \n",
    "\n",
    "                test_start = y_test_lenght*word_index\n",
    "                prediction_test_list.append(prediction_test[test_start:test_start + y_test_lenght])   \n",
    "\n",
    "        df_prediction = df.copy()\n",
    "\n",
    "        df_prediction['validate_prediction'] = prediction_valid_list\n",
    "        df_prediction['test_prediction'] = prediction_test_list\n",
    "        return df_prediction\n",
    "    \n",
    "    def data_visualization_prediction(self, df, word_list, first_period,y_train_lenght,y_valid_lenght,y_test_lenght, n_steps):\n",
    "        '''\n",
    "        A function that visualizes the prediction results\n",
    "        '''\n",
    "        name_word=[]\n",
    "        last_period = len(df.iloc[1,1]) - first_period\n",
    "\n",
    "        plt.style.use('seaborn-darkgrid')\n",
    "        # create a color palette\n",
    "        palette, palette2, palette3 = plt.get_cmap('Set1'),plt.get_cmap('tab20'),plt.get_cmap('Dark2')\n",
    "\n",
    "        x_0 = np.array([i+1 for i in range(len(df.iloc[1,1]))])\n",
    "        x_1 = np.array([i+1 for i in range(n_steps + y_train_lenght,n_steps + y_train_lenght+y_valid_lenght)])\n",
    "        x_2 = np.array([i+1 for i in range(n_steps + y_train_lenght+y_valid_lenght,n_steps + y_train_lenght+y_valid_lenght+y_test_lenght)])\n",
    "\n",
    "        for i in range(len(word_list)):\n",
    "            plt.plot(df['tf-idf_after_norm'][df['word'] == word_list[i]].to_list()[0][first_period:],linestyle='dashed', color=palette(i),linewidth=1, alpha=0.9, label=word_list[i])\n",
    "            plt.plot(x_1,df['validate_prediction'][df['word'] == word_list[i]].tolist()[0],linestyle='dashdot',linewidth=1.5, alpha=1,color=palette2(i), label=word_list[i])\n",
    "            plt.plot(x_2,df['test_prediction'][df['word'] == word_list[i]].tolist()[0],linestyle='solid',linewidth=1, alpha=1,color=palette3(i), label=word_list[i])\n",
    "            name_word.append(word_list[i])\n",
    "\n",
    "        # Add legend\n",
    "        plt.legend(loc=1, ncol=1)\n",
    "\n",
    "        plt.ylabel('prediction')\n",
    "        plt.xlabel('w')\n",
    "\n",
    "        plt.legend(['tf_idf_normalize','validate_prediction','test_prediction'])\n",
    "        plt.title('Visualization of words prediction-word name: '+\" \".join(name_word))\n",
    "\n",
    "        plt.gcf().set_size_inches(15.5, 10.5, forward=True)  \n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "    def average_vectors(self,df,number_clusters, name_cluster):\n",
    "        '''\n",
    "        Function calculates the average of vectors for each cluster. \n",
    "        The calculation is done by schematic each of the columns of the word vector and dividing by 2.\n",
    "        '''\n",
    "        dict_data={}\n",
    "        for number_cluster in range(number_clusters):\n",
    "            # All data for a single cluster\n",
    "            df_cluster = df[df[name_cluster] ==number_cluster]\n",
    "            # Get TF IDF normalization vectors for a single cluster\n",
    "            dataset = pd.DataFrame(df_cluster['tf-idf_after_norm'].to_list()).iloc[:,:].values\n",
    "\n",
    "            dict_data.update({'cluster '+str(number_cluster):[sum(list_values)/2 for list_values in zip(*dataset.tolist())]})\n",
    "        return dict_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
